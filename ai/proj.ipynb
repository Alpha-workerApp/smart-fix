{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozi8JRJRorIo"
      },
      "source": [
        "##BASIC CHATBOT (Alter based on requirments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw60ta_gWV-n"
      },
      "outputs": [],
      "source": [
        "#types of flan-t5 model\n",
        "'''\n",
        "flan-t5-small\n",
        "flan-t5-base\n",
        "flan-t5-large\n",
        "flan-t5-xl\n",
        "flan-t5-xxl\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgiPIaGdWftW",
        "outputId": "5b2531f0-43c6-4a64-b4f7-79d5c3f1edb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated responses:\n",
            "Bot: I would suggest getting in touch with an electrician.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"google/flan-t5-large\"\n",
        "# model_name = \"google/flan-t5-base\"\n",
        "# model_name=\"google/flan-t5-xxl\"\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,use_fast=True)\n",
        "\n",
        "conversation_history=[]\n",
        "\n",
        "def generate_text(prompt, conversation_history, max_length=400, num_return_sequences=1):\n",
        "    if not model or not tokenizer:\n",
        "        raise ValueError(\"Model and tokenizer are not loaded. Please load them before using this function.\")\n",
        "\n",
        "    conversation_history.append(prompt)\n",
        "    full_conversation='\\n'.join(conversation_history)\n",
        "\n",
        "    inputs = tokenizer(full_conversation, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          max_length=max_length,\n",
        "          num_beams=2,#6\n",
        "          no_repeat_ngram_size=1,\n",
        "          num_return_sequences=num_return_sequences,\n",
        "          length_penalty=1.8,\n",
        "          early_stopping=True\n",
        "      )\n",
        "\n",
        "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return generated_texts\n",
        "\n",
        "chatbot_role = \"Technician Support Chatbot\"\n",
        "chatbot_instructions = \"\"\"\n",
        "You are a technician support chatbot.\n",
        "Your primary goal is to assist users by identifying their technical problems and suggesting suitable technicians.\n",
        "Additionally, you should handle other queries related to the application.\n",
        "\n",
        "Your responses should be:\n",
        "* Clear and concise: Avoid unnecessary jargon or overly complex explanations.\n",
        "* Helpful and informative: Provide relevant and useful information to the user.\n",
        "* Friendly and approachable: Maintain a positive and welcoming tone.\n",
        "* Accurate and reliable: Ensure your responses are correct and trustworthy.\n",
        "* Contextual: Consider the context of the conversation and tailor your responses accordingly.\n",
        "* Domain-specific: Keep your responses within the application domain.\n",
        "\n",
        "Your domain includes the following technician services:\n",
        "- Electrician\n",
        "- Plumber\n",
        "- Carpenter\n",
        "- Cleaner\n",
        "- Painter\n",
        "- AC & HVAC Technician\n",
        "- Sewage and Drain Cleaner\n",
        "- Welding and Metalwork\n",
        "- Emergency Generator and Invertor Maintenance Worker\n",
        "\n",
        "For example:\n",
        "- **User:** My tap is not working.\n",
        "- **Chatbot:** It sounds like you're experiencing a plumbing issue. For a problem with your tap, I would recommend contacting a plumber. Plumbers are skilled in repairing and maintaining various plumbing fixtures and can help ensure everything is functioning correctly. Would you like me to assist you in finding a plumber in your area?\n",
        "\n",
        "- **User:** My tube light is not working.\n",
        "- **Chatbot:** It sounds like you're having an issue with your tube light. For problems like this, I would suggest getting in touch with an electrician. Electricians are trained to handle electrical issues and can help diagnose and fix the problem efficiently. Would you like me to assist you in finding an electrician nearby?\n",
        "\n",
        "- **User:** My inverter is not working.\n",
        "- **Chatbot:** It sounds like you're having an issue with your inverter. For problems like this, I would recommend contacting an Emergency Generator and Invertor Maintenance Worker. These professionals are trained to handle issues with inverters and can help diagnose and fix the problem efficiently. Would you like me to assist you in finding a technician nearby?\n",
        "\n",
        "Remember to stay focused on providing application-related support and technician recommendations.\n",
        "\"\"\"\n",
        "\n",
        "user_input = \"light is not glowing \"\n",
        "prompt = f\"{chatbot_instructions}\\n\\n**Role:** {chatbot_role}\\n\\n**User:** {user_input}\\n\\n**Chatbot:**\"\n",
        "generated_texts = generate_text(prompt,conversation_history)\n",
        "print(\"Generated responses:\")\n",
        "\n",
        "for text in generated_texts:\n",
        "    print(f\"Bot: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivV8JKGuXwuB",
        "outputId": "e14cb2da-14cc-41f6-9e51-f6f61db3ed4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1020 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated responses:\n",
            "Bot: I can help you with that. What kind of paint do u want to use?\n"
          ]
        }
      ],
      "source": [
        "user_input = \"i need to paint my house \"\n",
        "prompt = f\"{chatbot_instructions}\\n\\n**Role:** {chatbot_role}\\n\\n**User:** {user_input}\\n\\n**Chatbot:**\"\n",
        "generated_texts = generate_text(prompt,conversation_history)\n",
        "print(\"Generated responses:\")\n",
        "\n",
        "for text in generated_texts:\n",
        "    print(f\"Bot: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcltVOVZbcOm",
        "outputId": "18805941-7d3a-4d92-8be5-3e8d88551ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated responses:\n",
            "Bot: It sounds like you're having an issue with your washing machine. For problems such as this, I would suggest contacting one of the following technicians:\n"
          ]
        }
      ],
      "source": [
        "user_input = \"my washing machine is not working\"\n",
        "prompt = f\"{chatbot_instructions}\\n\\n**Role:** {chatbot_role}\\n\\n**User:** {user_input}\\n\\n**Chatbot:**\"\n",
        "generated_texts = generate_text(prompt,conversation_history)\n",
        "print(\"Generated responses:\")\n",
        "\n",
        "for text in generated_texts:\n",
        "    print(f\"Bot: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZgZUOD3rUL8",
        "outputId": "cfa3f259-84f0-4fe8-ed3f-888ec6e31eba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated responses:\n",
            "Bot: It sounds like you're having an issue with your inverter. For problems such as this, I would recommend contacting Emergency Generator and Inveror Maintenance Workers\n"
          ]
        }
      ],
      "source": [
        "user_input = \"my invertor is not working \"\n",
        "prompt = f\"{chatbot_instructions}\\n\\n**Role:** {chatbot_role}\\n\\n**User:** {user_input}\\n\\n**Chatbot:**\"\n",
        "generated_texts = generate_text(prompt,conversation_history)\n",
        "print(\"Generated responses:\")\n",
        "\n",
        "for text in generated_texts:\n",
        "    print(f\"Bot: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsXyqiMZgTGT",
        "outputId": "3b234c4e-31a4-4e7c-d540-aae554fabcdd"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    user_input = input(\"Enter your query (type 'quit' to exit): \")\n",
        "    if user_input.lower() == \"quit\":\n",
        "        print(\"Exiting the chatbot. Have a great day!\")\n",
        "        break\n",
        "\n",
        "    prompt = f\"{chatbot_instructions}\\n\\n**Role:** {chatbot_role}\\n\\n**User:** {user_input}\\n\\n**Chatbot:**\"\n",
        "    generated_texts = generate_text(prompt,conversation_history)\n",
        "    print(\"Generated responses:\")\n",
        "\n",
        "    for text in generated_texts:\n",
        "        print(f\"Bot: {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM3pJJhgq5sl"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "############################################################################### sentiment analysis ###############################################################\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hli2bV6iw4o_"
      },
      "source": [
        "##MODEL 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFpNTHOlISQ0",
        "outputId": "0ad2faa4-9096-4560-946d-09883f0f8920"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positive\n",
            "Professionalism - Was the technician polite and respectful? (1 to 5): 3\n",
            "Expertise – Did the technician demonstrate knowledge and skill? (1 to 5): 3\n",
            "Timeliness – Was the service completed on time? (1 to 5): 3\n",
            "Communication – Did the technician clearly explain the issue and solution? (1 to 5): 3\n",
            "Work Quality – Was the issue resolved effectively and satisfactorily? (1 to 5): 3\n",
            "3.0\n",
            "Neutral\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "sia=SentimentIntensityAnalyzer()\n",
        "def analyze_sentiment(text):\n",
        "  score=sia.polarity_scores(str(text))\n",
        "  if score['compound']>=0.05:\n",
        "    return 'Positive'\n",
        "  elif score['compound']<=-0.05:\n",
        "    return 'Negative'\n",
        "  else:\n",
        "    return 'Neutral'\n",
        "\n",
        "inputs='The technician work well'\n",
        "text_senti=analyze_sentiment(inputs)\n",
        "print(text_senti)\n",
        "\n",
        "aspect1=int(input('Professionalism - Was the technician polite and respectful? (1 to 5): '))\n",
        "aspect2=int(input('Expertise – Did the technician demonstrate knowledge and skill? (1 to 5): '))\n",
        "aspect3=int(input('Timeliness – Was the service completed on time? (1 to 5): '))\n",
        "aspect4=int(input('Communication – Did the technician clearly explain the issue and solution? (1 to 5): '))\n",
        "aspect5=int(input('Work Quality – Was the issue resolved effectively and satisfactorily? (1 to 5): '))\n",
        "aspect_score=(aspect1+aspect2+aspect3+aspect4+aspect5)/5\n",
        "print(aspect_score)\n",
        "\n",
        "if text_senti=='Positive' and aspect_score>=4:\n",
        "  print('Positive')\n",
        "elif text_senti=='Negative' and aspect_score<=2:\n",
        "  print('Negative')\n",
        "else:\n",
        "  print('Neutral')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AxxaZYTpxA1X"
      },
      "source": [
        "##MODEL 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krs05AzLky0b",
        "outputId": "809abcfd-6f15-4d14-94b3-521a97602dd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': '4 stars', 'score': 0.5136815905570984}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "print(sentiment_pipeline(\"The technician work well\"))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r1rmyaP50W1x"
      },
      "source": [
        "##MODEL 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSm8nKQqy27a",
        "outputId": "8e746e02-272d-44e5-8e91-796a41ef19b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Sentiment: positive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(\"The technician work well\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "output = outputs.logits\n",
        "\n",
        "# Get the predicted class index\n",
        "predicted_class_index = torch.argmax(output, dim=1).item()\n",
        "\n",
        "# Map the class index to the sentiment label\n",
        "sentiment_labels = ['negative', 'neutral', 'positive']\n",
        "predicted_sentiment = sentiment_labels[predicted_class_index]\n",
        "\n",
        "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f4W-eS_gyQFw"
      },
      "source": [
        "##MODEL 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPH8rijLwdwG",
        "outputId": "79b6f22c-1e54-45bd-9f97-d0cf4abcb93d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.99980229139328}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "print(sentiment_pipeline(\"The technician work well\"))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LcnSiiENyXXG"
      },
      "source": [
        "##MODEL 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyzE9SeHyOV0",
        "outputId": "a2398790-5276-4834-fe6e-250dae319dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment(polarity=0.0, subjectivity=0.0)\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "print(TextBlob(\"The technician didnt work well\").sentiment)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_09vdRgOyjbq"
      },
      "source": [
        "##MODEL 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq2qpOaNyfXh",
        "outputId": "91b41360-d691-4b61-816b-b7dbd272663c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "negative\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import socket\n",
        "\n",
        "# Increase the timeout value\n",
        "socket.setdefaulttimeout(30)  # Set timeout to 30 seconds\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "result = classifier(\n",
        "    \"The technician didn't work well\",\n",
        "    candidate_labels=[\"positive\", \"negative\", \"neutral\"],\n",
        ")\n",
        "\n",
        "# Print only the highest scoring label\n",
        "print(result[\"labels\"][0])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdcMn581aSS"
      },
      "source": [
        "##MODEL 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvkaeHHN1wDj"
      },
      "outputs": [],
      "source": [
        "!pip install -q flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq6Ca_kO0rz-",
        "outputId": "ab9e5b96-ab05-4a89-ad4e-624aa9bca0d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NEGATIVE\n"
          ]
        }
      ],
      "source": [
        "from flair.nn import Classifier\n",
        "from flair.data import Sentence\n",
        "\n",
        "sentence = Sentence(\"The technician didn't work well\")\n",
        "classifier = Classifier.load('sentiment')\n",
        "classifier.predict(sentence)\n",
        "\n",
        "predicted_label = sentence.labels[0].value\n",
        "print(predicted_label)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ0kZBSG3B0v"
      },
      "source": [
        "##MODEL 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2krnrCL3AuK",
        "outputId": "8bbceb36-55c0-45b0-81b9-3f8508ae822b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'label': '4 stars', 'score': 0.5136815905570984}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "print(sentiment_pipeline(\"The technician work well\"))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
